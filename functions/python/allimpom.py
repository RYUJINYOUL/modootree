import requests
import json
import os
import traceback
import time
import re
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
from typing import Dict, List, Tuple, Optional
from enum import Enum
from datetime import datetime, timedelta

from google import genai
from google.genai import types
from flask import Flask, request, Response, jsonify
from flask_cors import CORS

# Trafilatura for fast web scraping
try:
    import trafilatura
    HAS_TRAFILATURA = True
    print("âœ… Trafilatura ë¡œë“œ ì™„ë£Œ")
except ImportError:
    HAS_TRAFILATURA = False
    print("âš ï¸ Trafilaturaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install trafilatura")

app = Flask(__name__)

# CORS ì„¤ì • (ëª¨ë“  ì¶œì²˜ í—ˆìš©)
CORS(app, 
     resources={r"/*": {"origins": "*"}},
     allow_headers=["Content-Type", "Authorization"],
     methods=["GET", "POST", "OPTIONS"],
     expose_headers=["Content-Type"],
     max_age=3600)

# ===== í™˜ê²½ ë³€ìˆ˜ =====
SERPER_KEY = os.environ.get("SERPER_KEY")
NAVER_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_SECRET = os.environ.get("NAVER_CLIENT_SECRET")
GEMINI_KEY = os.environ.get("GEMINI_API_KEY")

client = None
if GEMINI_KEY:
    try:
        client = genai.Client(api_key=GEMINI_KEY)
        print("âœ… Gemini Client ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ Gemini Client ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ê³„ì† ì§„í–‰ (ì•±ì€ ì‹œì‘í•´ì•¼ í•¨)
else:
    print("âš ï¸ GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

SEARCH_CONFIG = {
    "naver": {
        "url": "https://openapi.naver.com/v1/search/local.json",
        "headers": {
            "X-Naver-Client-Id": NAVER_ID,
            "X-Naver-Client-Secret": NAVER_SECRET,
        },
    },
    "google": {
        "url": "https://google.serper.dev/search",
        "headers": {"X-API-KEY": SERPER_KEY, "Content-Type": "application/json"},
    },
}

class SearchCategory(Enum):
    RESTAURANT = "restaurant"
    CAFE = "cafe"
    ACCOMMODATION = "accommodation"
    SHOPPING = "shopping"
    NEWS = "news"
    PRODUCT = "product"
    ACTIVITY = "activity"
    GENERAL = "general"

def classify_query(query: str) -> Tuple[SearchCategory, str]:
    """ì¿¼ë¦¬ ë¶„ë¥˜"""
    q = query.lower()
    keywords = {
        SearchCategory.RESTAURANT: ["ë§›ì§‘", "ìŒì‹ì ", "ë ˆìŠ¤í† ë‘", "ë¨¹ì„ê³³", "ì‹ë‹¹"],
        SearchCategory.CAFE: ["ì¹´í˜", "ì»¤í”¼", "ë””ì €íŠ¸", "ë² ì´ì»¤ë¦¬"],
        SearchCategory.ACCOMMODATION: ["ìˆ™ì†Œ", "í˜¸í…”", "ëª¨í…”", "íœì…˜", "ë¦¬ì¡°íŠ¸"],
        SearchCategory.NEWS: ["ë‰´ìŠ¤", "ê¸°ì‚¬", "ì†Œì‹"],
        SearchCategory.SHOPPING: ["ì‡¼í•‘", "êµ¬ë§¤"],
        SearchCategory.PRODUCT: ["ì œí’ˆ", "ìƒí’ˆ", "ì¶”ì²œ"],
        SearchCategory.ACTIVITY: ["ì²´í—˜", "ê´€ê´‘", "ì—¬í–‰"],
    }
    
    for category, kws in keywords.items():
        if any(kw in q for kw in kws):
            clean = query
            for word in ["ì¶”ì²œ", "ì•Œë ¤ì¤˜", "ì°¾ì•„ì¤˜", "ê²€ìƒ‰", "í•´ì¤˜"]:
                clean = clean.replace(word, "").strip()
            return category, clean
    
    return SearchCategory.GENERAL, query

def fetch_api_data(source: str, query: str) -> Dict:
    """API ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    config = SEARCH_CONFIG.get(source)
    if not config:
        return {"source": source, "error": "config not found"}
    
    try:
        if source == "naver":
            r = requests.get(
                config["url"],
                headers=config["headers"],
                params={"query": query, "display": 10},
                timeout=5
            )
        elif source == "google":
            r = requests.post(
                config["url"],
                headers=config["headers"],
                json={"q": query, "num": 10},
                timeout=5
            )
        else:
            return {"source": source, "error": "unknown"}
        
        r.raise_for_status()
        return {"source": source, "data": r.json()}
    
    except Exception as e:
        print(f"âš ï¸ {source} API ì—ëŸ¬: {e}")
        return {"source": source, "error": str(e)}

def filter_search_results(raw_results: List[Dict]) -> List[Dict]:
    """ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ ë° ë§í¬ ì¶”ì¶œ"""
    cleaned = []
    
    for result in raw_results:
        source = result.get("source")
        data = result.get("data", {})
        
        if source == "naver":
            items = data.get("items", [])
            for item in items[:5]:
                title = re.sub(r'<[^>]+>', '', item.get("title", ""))
                desc = re.sub(r'<[^>]+>', '', item.get("description", ""))
                
                cleaned.append({
                    "source": source,
                    "title": title,
                    "link": item.get("link", ""),
                    "snippet": desc,
                    "address": item.get("address", ""),
                })
        
        elif source == "google":
            items = data.get("organic", [])
            for item in items[:5]:
                cleaned.append({
                    "source": source,
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                })
    
    return cleaned

def scrape_page(url: str, max_chars: int = 500) -> Dict:
    """ë‹¨ì¼ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ (Trafilatura ì‚¬ìš©)"""
    if not HAS_TRAFILATURA:
        return {
            "url": url,
            "summary": "ìŠ¤í¬ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
            "success": False
        }
    
    try:
        # HTML ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url, timeout=5, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()
        
        # Trafilaturaë¡œ ë³¸ë¬¸ ì¶”ì¶œ
        text = trafilatura.extract(
            response.text,
            include_comments=False,
            include_tables=False,
            no_fallback=False
        )
        
        if not text or len(text.strip()) < 50:
            return {
                "url": url,
                "summary": "ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "success": False
            }
        
        # ìš”ì•½
        summary = text[:max_chars].strip()
        if len(text) > max_chars:
            summary += "..."
        
        return {
            "url": url,
            "summary": summary,
            "full_text": text[:1500],  # LLMì— ë³´ë‚¼ ì „ì²´ í…ìŠ¤íŠ¸
            "success": True
        }
    
    except Exception as e:
        print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ({url}): {e}")
        return {
            "url": url,
            "summary": f"í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)[:50]}",
            "success": False
        }

def scrape_multiple_pages(urls: List[str], max_workers: int = 5) -> List[Dict]:
    """ë³‘ë ¬ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {
            executor.submit(scrape_page, url): url 
            for url in urls[:10]  # ìµœëŒ€ 10ê°œ
        }
        
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                result = future.result(timeout=7)
                results.append(result)
            except Exception as e:
                print(f"âŒ ìŠ¤í¬ë˜í•‘ íƒ€ì„ì•„ì›ƒ: {url}")
                results.append({
                    "url": url,
                    "summary": "íƒ€ì„ì•„ì›ƒ",
                    "success": False
                })
    
    return results

def sse_format(data: Dict) -> str:
    """SSE í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

@app.route("/stream", methods=["POST", "OPTIONS"])
def stream_search():
    """ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰"""
    # OPTIONS ìš”ì²­ ì²˜ë¦¬ (CORS preflight)
    if request.method == "OPTIONS":
        print("ğŸ“¨ OPTIONS ìš”ì²­ ìˆ˜ì‹ ")
        response = Response("", status=200)
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = "POST, OPTIONS"
        response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization"
        response.headers["Access-Control-Max-Age"] = "3600"
        return response

    print(f"ğŸ“¨ POST ìš”ì²­ ìˆ˜ì‹ ")
    req = request.get_json(silent=True) or {}
    user_input = req.get("query", "").strip()
    
    if not user_input:
        return Response(
            sse_format({"error": "query í•„ìˆ˜"}),
            mimetype="text/event-stream"
        )

    def generate():
        start = time.time()
        
        try:
            # ===== 1ï¸âƒ£ ì¿¼ë¦¬ ë¶„ë¥˜ =====
            yield sse_format({
                "stage": "classify",
                "status": "started",
                "message": "ğŸ” ê²€ìƒ‰ ì¤€ë¹„ ì¤‘..."
            })
            
            category, clean_query = classify_query(user_input)
            
            yield sse_format({
                "stage": "classify",
                "status": "finished",
                "category": category.value,
                "message": f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {category.value}"
            })

            # ===== 2ï¸âƒ£ ê²€ìƒ‰ API (ë³‘ë ¬) =====
            yield sse_format({
                "stage": "search",
                "status": "started",
                "message": "ğŸ” ê²€ìƒ‰ ì¤‘...",
                "progress": 10
            })
            
            raw_results = []
            with ThreadPoolExecutor(max_workers=2) as ex:
                naver_fut = ex.submit(fetch_api_data, "naver", clean_query)
                
                try:
                    naver_result = naver_fut.result(timeout=3)
                    raw_results.append(naver_result)
                    print(f"âœ… NAVER ê²€ìƒ‰ ì™„ë£Œ")
                except TimeoutError:
                    print(f"â° NAVER API ì‹œê°„ ì´ˆê³¼")
                
                if SERPER_KEY:
                    google_fut = ex.submit(fetch_api_data, "google", clean_query)
                    try:
                        google_result = google_fut.result(timeout=3)
                        raw_results.append(google_result)
                        print(f"âœ… Google ê²€ìƒ‰ ì™„ë£Œ")
                    except TimeoutError:
                        print(f"â° Google API ì‹œê°„ ì´ˆê³¼")
            
            yield sse_format({
                "stage": "search",
                "status": "finished",
                "message": "âœ… ê²€ìƒ‰ ì™„ë£Œ",
                "progress": 25
            })

            # ===== 3ï¸âƒ£ ê²°ê³¼ í•„í„°ë§ =====
            cleaned = filter_search_results(raw_results)
            
            if not cleaned:
                yield sse_format({
                    "stage": "error",
                    "error": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                })
                return
            
            yield sse_format({
                "stage": "filter",
                "status": "finished",
                "count": len(cleaned),
                "message": f"ğŸ“‹ {len(cleaned)}ê°œ ê²°ê³¼ ë°œê²¬",
                "progress": 30
            })

            # ===== 4ï¸âƒ£ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ (ë³‘ë ¬) =====
            if HAS_TRAFILATURA:
                yield sse_format({
                    "stage": "scrape",
                    "status": "started",
                    "message": "ğŸ“„ í˜ì´ì§€ ë¶„ì„ ì¤‘...",
                    "progress": 35
                })
                
                # ë§í¬ ì¶”ì¶œ
                links = [item["link"] for item in cleaned if item.get("link")]
                
                # ë³‘ë ¬ ìŠ¤í¬ë˜í•‘
                scraped_data = scrape_multiple_pages(links, max_workers=5)
                
                success_count = sum(1 for s in scraped_data if s["success"])
                
                yield sse_format({
                    "stage": "scrape",
                    "status": "finished",
                    "count": success_count,
                    "total": len(scraped_data),
                    "message": f"âœ… {success_count}/{len(scraped_data)}ê°œ í˜ì´ì§€ ë¶„ì„ ì™„ë£Œ",
                    "progress": 60
                })
            else:
                scraped_data = []
                yield sse_format({
                    "stage": "scrape",
                    "status": "skipped",
                    "message": "âš ï¸ ìŠ¤í¬ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ",
                    "progress": 60
                })

            # ===== 5ï¸âƒ£ LLM ìš”ì•½ (ìŠ¤íŠ¸ë¦¬ë°) =====
            yield sse_format({
                "stage": "synthesis",
                "status": "started",
                "message": "âœ¨ ë‹µë³€ ìƒì„± ì¤‘...",
                "progress": 65
            })
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            context_data = []
            
            # ìŠ¤í¬ë˜í•‘ ì„±ê³µí•œ ë°ì´í„° ìš°ì„ 
            for item in scraped_data:
                if item["success"]:
                    context_data.append({
                        "url": item["url"],
                        "content": item["full_text"]
                    })
            
            # snippetë„ ì¶”ê°€ (ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ í´ë°±)
            for item in cleaned[:10]:
                context_data.append({
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", ""),
                    "url": item.get("link", "")
                })
            
            prompt = f"""ì‚¬ìš©ì ì¿¼ë¦¬: {user_input}

ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:

{json.dumps(context_data[:10], ensure_ascii=False, indent=2)}

ë‹µë³€ í˜•ì‹:
- 5~7ê°œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±
- í•µì‹¬ ì •ë³´ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´
- êµ¬ì²´ì ì¸ ì •ë³´ í¬í•¨ (ì£¼ì†Œ, ê°€ê²©, í‰ì  ë“±)"""
            
            # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° LLM ì‘ë‹µ
            full_answer = ""
            chunk_count = 0
            
            for chunk in client.models.generate_content_stream(
                model="gemini-2.0-flash",
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=600
                )
            ):
                if chunk.text:
                    full_answer += chunk.text
                    chunk_count += 1
                    
                    # ì‹¤ì‹œê°„ ì²­í¬ ì „ì†¡
                    yield sse_format({
                        "stage": "synthesis",
                        "status": "streaming",
                        "chunk": chunk.text,
                        "partial_answer": full_answer,
                        "progress": min(65 + (chunk_count * 2), 95)
                    })
            
            yield sse_format({
                "stage": "synthesis",
                "status": "finished",
                "message": "âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ",
                "progress": 100
            })

            # ===== 6ï¸âƒ£ ì™„ë£Œ =====
            duration = round(time.time() - start, 2)
            
            yield sse_format({
                "stage": "complete",
                "status": "finished",
                "category": category.value,
                "duration_sec": duration,
                "answer_summary": full_answer,
                "sources": [
                    {
                        "title": item.get("title", ""),
                        "snippet": item.get("snippet", "")[:150],
                        "link": item.get("link", ""),
                        "source": item.get("source", "")
                    }
                    for item in cleaned[:10]
                ],
                "scraped_count": success_count if HAS_TRAFILATURA else 0,
                "message": f"âœ… ì™„ë£Œ ({duration}ì´ˆ)"
            })

        except Exception as e:
            trace = traceback.format_exc()
            print(f"âŒ ì—ëŸ¬: {e}\n{trace}")
            yield sse_format({
                "stage": "error",
                "error": str(e),
                "message": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}"
            })

    return Response(
        generate(),
        mimetype="text/event-stream",
        headers={
            "Access-Control-Allow-Origin": "*",
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
        }
    )

@app.route("/health", methods=["GET"])
def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return jsonify({
        "status": "ok",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "gemini": client is not None,
            "naver": NAVER_ID is not None,
            "serper": SERPER_KEY is not None,
            "trafilatura": HAS_TRAFILATURA,
        },
    }), 200

# ì•± ì‹œì‘ ì‹œ ì •ë³´ ì¶œë ¥
print("ğŸš€ ìµœì í™”ëœ AllimPom ì„œë²„ ì´ˆê¸°í™”...")
print(f"âœ… Gemini: {'ì—°ê²°ë¨' if client else 'ì—°ê²° ì•ˆ ë¨'}")
print(f"âœ… NAVER: {'ì—°ê²°ë¨' if NAVER_ID else 'ì—°ê²° ì•ˆ ë¨'}")
print(f"âœ… Trafilatura: {'í™œì„±í™”' if HAS_TRAFILATURA else 'ë¹„í™œì„±í™”'}")
print(f"ğŸ“ ë“±ë¡ëœ ë¼ìš°íŠ¸:")
for rule in app.url_map.iter_rules():
    print(f"  - {rule.rule} [{', '.join(rule.methods)}]")

if __name__ == "__main__":
    print("ğŸš€ Flask ê°œë°œ ì„œë²„ë¡œ ì‹¤í–‰...")
    app.run(
        host="0.0.0.0",
        port=int(os.environ.get("PORT", 8080)),
        debug=False,
        threaded=True
    )
