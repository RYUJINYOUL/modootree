import os
import json
import re
import time
import traceback
from datetime import date, datetime, timedelta, timezone
from typing import TypedDict, List, Literal, Optional
from collections import OrderedDict
from threading import Lock

import uvicorn
import google.generativeai as genai
import firebase_admin
from firebase_admin import credentials, auth, firestore
from fastapi import FastAPI, Request, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from flask import Flask, request, Response, jsonify
from flask_cors import CORS

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from fag_data import FAQ_DATA

# ===== ë©”ëª¨ë¦¬ ìºì‹œ (TTL: 3ì‹œê°„) =====
class MemoryCache:
    """Thread-safe ë©”ëª¨ë¦¬ ìºì‹œ (TTL ì§€ì›)"""
    def __init__(self, ttl_seconds: int = 10800, max_size: int = 1000):
        self.cache: OrderedDict = OrderedDict()
        self.ttl = ttl_seconds  # ê¸°ë³¸ 3ì‹œê°„ (10800ì´ˆ)
        self.max_size = max_size
        self.lock = Lock()
    
    def _generate_key(self, query: str) -> str:
        """ì¿¼ë¦¬ë¥¼ ì •ê·œí™”í•˜ì—¬ ìºì‹œ í‚¤ ìƒì„±"""
        return query.strip().lower()
    
    def get(self, query: str) -> Optional[dict]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        key = self._generate_key(query)
        with self.lock:
            if key in self.cache:
                cached_data = self.cache[key]
                # TTL ì²´í¬
                if time.time() - cached_data["timestamp"] < self.ttl:
                    # LRU: ìµœê·¼ ì‚¬ìš©í•œ í•­ëª©ì„ ë§¨ ë’¤ë¡œ ì´ë™
                    self.cache.move_to_end(key)
                    print(f"ğŸ’¾ ìºì‹œ íˆíŠ¸: '{query}' (ë§Œë£Œê¹Œì§€ {self.ttl - (time.time() - cached_data['timestamp']):.0f}ì´ˆ)")
                    return cached_data["data"]
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                    print(f"â° ìºì‹œ ë§Œë£Œ: '{query}'")
                    del self.cache[key]
        return None
    
    def set(self, query: str, data: dict):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        key = self._generate_key(query)
        with self.lock:
            # ìµœëŒ€ í¬ê¸° ì²´í¬ (LRU: ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì‚­ì œ)
            if len(self.cache) >= self.max_size:
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]
                print(f"ğŸ—‘ï¸ ìºì‹œ ìš©ëŸ‰ ì´ˆê³¼: '{oldest_key}' ì‚­ì œ")
            
            self.cache[key] = {
                "data": data,
                "timestamp": time.time()
            }
            print(f"ğŸ’¾ ìºì‹œ ì €ì¥: '{query}' (ì´ {len(self.cache)}ê°œ)")
    
    def clear(self):
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        with self.lock:
            self.cache.clear()
            print("ğŸ—‘ï¸ ìºì‹œ ì „ì²´ ì‚­ì œ")
    
    def get_stats(self) -> dict:
        """ìºì‹œ í†µê³„"""
        with self.lock:
            total = len(self.cache)
            expired = sum(
                1 for item in self.cache.values() 
                if time.time() - item["timestamp"] >= self.ttl
            )
            return {
                "total": total,
                "valid": total - expired,
                "expired": expired,
                "ttl_hours": self.ttl / 3600
            }

# ê¸€ë¡œë²Œ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ (TTL: 3ì‹œê°„, ìµœëŒ€ 1000ê°œ ì¿¼ë¦¬)
memory_cache = MemoryCache(ttl_seconds=10800, max_size=1000)

# --- ìƒìˆ˜ ë° í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ---

cred_path = "serviceAccountKey.json"
cred_json_str = os.environ.get('FIREBASE_SERVICE_ACCOUNT_JSON')
DAILY_CHAT_LIMIT = 200

if os.path.exists(cred_path):
    cred = credentials.Certificate(cred_path)
    print("âœ… ë¡œì»¬ ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ë¡œ Firebase ì´ˆê¸°í™”")
elif cred_json_str:
    try:
        cred_json = json.loads(cred_json_str)
        cred = credentials.Certificate(cred_json)
        print("âœ… í™˜ê²½ ë³€ìˆ˜(JSON)ë¡œ Firebase ì´ˆê¸°í™”")
    except Exception as e:
        print(f"âŒ Firebase ì¸ì¦ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        cred = None
else:
    # ê°œë³„ í™˜ê²½ ë³€ìˆ˜ë¡œ Firebase ì´ˆê¸°í™” ì‹œë„
    project_id = os.environ.get('FIREBASE_PROJECT_ID')
    client_email = os.environ.get('FIREBASE_CLIENT_EMAIL')
    private_key_base64 = os.environ.get('FIREBASE_PRIVATE_KEY_BASE64')
    
    if project_id and client_email and private_key_base64:
        try:
            import base64
            private_key = base64.b64decode(private_key_base64).decode('utf-8')
            
            cred_dict = {
                "type": "service_account",
                "project_id": project_id,
                "client_email": client_email,
                "private_key": private_key,
                "private_key_id": "",
                "client_id": "",
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": "https://oauth2.googleapis.com/token",
                "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs"
            }
            
            cred = credentials.Certificate(cred_dict)
            print("âœ… ê°œë³„ í™˜ê²½ ë³€ìˆ˜ë¡œ Firebase ì´ˆê¸°í™”")
        except Exception as e:
            print(f"âŒ Firebase ê°œë³„ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            cred = None
    else:
        print("âš ï¸ Firebase ì„œë¹„ìŠ¤ ê³„ì • í™˜ê²½ ë³€ìˆ˜ ë¯¸ì„¤ì •.")
        cred = None

if cred:
    firebase_admin.initialize_app(cred)
    db = firestore.client()
else:
    db = None

GOOGLE_AI_KEY = os.getenv('GOOGLE_AI_KEY')
if not GOOGLE_AI_KEY:
    print("âš ï¸ GOOGLE_AI_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
genai.configure(api_key=GOOGLE_AI_KEY)

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---

def verify_firebase_token(id_token: str) -> dict:
    """Firebase ID í† í° ê²€ì¦"""
    try:
        decoded_token = auth.verify_id_token(id_token)
        return decoded_token
    except Exception as e:
        print(f"âŒ Firebase í† í° ê²€ì¦ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, 
            detail="ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ë§Œë£Œëœ ì¸ì¦ í† í°ì…ë‹ˆë‹¤."
        )

async def check_and_update_chat_limit(uid: str) -> dict:
    """ì¼ì¼ ì±„íŒ… í•œë„ í™•ì¸ ë° ì—…ë°ì´íŠ¸"""
    if not db:
        return {"canChat": True, "remainingChats": DAILY_CHAT_LIMIT}

    today = date.today().isoformat()
    limit_ref = db.collection('users').document(uid).collection('limits').document(today)
    
    @firestore.transactional
    def update_in_transaction(transaction):
        doc = limit_ref.get(transaction=transaction)
        
        if doc.exists:
            data = doc.to_dict()
            count = data.get('count', 0)
            
            if count >= DAILY_CHAT_LIMIT:
                return {"canChat": False, "remainingChats": 0}
            
            new_count = count + 1
            transaction.set(limit_ref, {'count': new_count, 'last_chat': firestore.SERVER_TIMESTAMP}, merge=True)
            return {"canChat": True, "remainingChats": DAILY_CHAT_LIMIT - new_count}
        else:
            new_count = 1
            transaction.set(limit_ref, {'count': new_count, 'created_at': firestore.SERVER_TIMESTAMP, 'last_chat': firestore.SERVER_TIMESTAMP})
            return {"canChat": True, "remainingChats": DAILY_CHAT_LIMIT - new_count}

    try:
        return update_in_transaction(db.transaction())
    except Exception as e:
        print(f"Error checking chat limit: {e}")
        return {"canChat": False, "remainingChats": 0}

# ğŸš¨ ë©”ëª¨ ì €ì¥ ë¡œì§ ì§‘ì¤‘ ê°œì„  (ë¡œê·¸ ì¶”ê°€ ë° DB êµ¬ì¡° ëª…ì‹œ)
def save_memos_to_firestore(uid: str, memo_items: List[dict], db: firestore.client) -> int:
    """
    Agentì—ì„œ ì¶”ì¶œí•œ ë©”ëª¨ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ Firestoreì— ì €ì¥í•©ë‹ˆë‹¤.
    êµ¬ì¡°: collections/memos/documents/{uid}/collections/user_memos/documents/{memo_id}
    """
    if not db:
        print("âŒ Firestore í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì €ì¥ ì‹¤íŒ¨.")
        return 0
        
    saved_count = 0
    
    # UIDë¥¼ ìµœìƒìœ„ Collectionì˜ Document IDë¡œ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìë³„ ë¶„ë¦¬
    user_memo_collection_ref = (
        db.collection('memos')
        .document(uid)
        .collection('user_memos')
    )
    
    for item in memo_items:
        try:
            # ì €ì¥í•  ë°ì´í„°
            memo_data = {
                "content": item.get("content", "ë‚´ìš© ì—†ìŒ"), 
                "created_at": firestore.SERVER_TIMESTAMP,
                "is_completed": False,
                "is_tomorrow": item.get("isTomorrow", False) 
            }
            
            # ìƒˆ ë¬¸ì„œ ì¶”ê°€ (ìë™ ID ìƒì„±)
            user_memo_collection_ref.add(memo_data)
            
            saved_count += 1
            print(f"âœ… UID {uid}ì— ë©”ëª¨ í•­ëª© ì €ì¥ ì™„ë£Œ: {item.get('content', '')[:30]}...")
            
        except Exception as e:
            # ê°œë³„ í•­ëª© ì €ì¥ ì‹¤íŒ¨ ì‹œ, ì˜¤ë¥˜ ë¡œê·¸ë¥¼ ë‚¨ê¸°ê³  ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ ì´ë™
            print(f"âŒ ë©”ëª¨ ì €ì¥ ì¤‘ Firestore ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue 
            
    return saved_count


# --- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° ìŠ¤í‚¤ë§ˆ ---

SYSTEM_INSTRUCTION_PERSONA = """
ë‹¹ì‹ ì€ ëª¨ë‘íŠ¸ë¦¬ì˜ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì£¼ìš” ì—­í• ì€ ì‚¬ìš©ìì™€ ì¹œê·¼í•˜ê³  ê³µê°ì ì¸ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì…ë‹ˆë‹¤.
ë‹¹ì‹ ì€ í•­ìƒ ì¹œì ˆí•˜ê³  ê³µê°ì ì¸ ë§íˆ¬ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
"""

SYSTEM_INSTRUCTION_MEMO_AGENT = """
**[CRITICAL INSTRUCTION]** ë‹¹ì‹ ì€ ì§€ê¸ˆ SAVE_MEMO ìš”ì²­ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.
**ë°˜ë“œì‹œ** ì£¼ì–´ì§„ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¼ ì‘ë‹µí•´ì•¼ í•˜ë©°, action í•„ë“œëŠ” "SAVE_MEMO"ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
**ì ˆëŒ€ë¡œ** ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì‘ë‹µí•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
userResponse í•„ë“œëŠ” í•„ìˆ˜ì´ë©°, ì´ëª¨ì§€ë‚˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

ì‚¬ìš©ìì˜ ìš”ì²­ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê° ì¼ì •ì„ ê°œë³„ í•­ëª©ìœ¼ë¡œ ë¶„ë¦¬í•´ì£¼ì„¸ìš”.
ê° ë©”ëª¨ í•­ëª©ì€ ë°˜ë“œì‹œ ì‹œê°„ê³¼ ë‚´ìš©ì„ í¬í•¨í•´ì•¼ í•˜ë©°, 'ë‚´ì¼'ì´ë‚˜ ë¯¸ë˜ ë‚ ì§œê°€ ì–¸ê¸‰ëœ ê²½ìš° isTomorrowë¥¼ trueë¡œ ì„¤ì •í•˜ì„¸ìš”.
"""

AGENT_SCHEMA = {
    "type": "object",
    "properties": {
        "action": {"type": "string", "description": "ì‚¬ìš©ìê°€ ìš”ì²­í•œ í–‰ë™ (SAVE_MEMO ë˜ëŠ” NONE ì¤‘ í•˜ë‚˜)"},
        "userResponse": {"type": "string", "description": "ì €ì¥ ì™„ë£Œ í›„ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ì¹œê·¼í•˜ê³  ê³µê°ì ì¸ ì‘ë‹µ ë©”ì‹œì§€."},
        "memoItems": {
            "type": "array",
            "description": "SAVE_MEMO ì•¡ì…˜ì¼ ë•Œ ì‚¬ìš©. ì‚¬ìš©ì ìš”ì²­ì—ì„œ ì¶”ì¶œëœ í•˜ë‚˜ ì´ìƒì˜ ê°œë³„ ë©”ëª¨ í•­ëª© ë¦¬ìŠ¤íŠ¸.",
            "items": {
                "type": "object",
                "properties": {
                    "content": {"type": "string", "description": "ê°œë³„ ë©”ëª¨ í•­ëª©ì˜ ë‚´ìš©."},
                    "isTomorrow": {"type": "boolean", "description": "'ë‚´ì¼' í‚¤ì›Œë“œë‚˜ ë¯¸ë˜ ë‚ ì§œ ì–¸ê¸‰ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ true"}
                },
                "required": ['content', 'isTomorrow']
            }
        },
    },
    "required": ['action', 'userResponse']
}

# âœ… FAQ í‚¤ì›Œë“œ ê°œì„  (ê³µë°± ì œê±° ë²„ì „ ì¶”ê°€)
FAQ_KEYWORD_MAP = {
    "ì‚¬ì—°íˆ¬í‘œ": "ì œê°€ ëŒ€í™”í•œ ë‚´ìš©ì„ AI ì‚¬ì—° íˆ¬í‘œë¡œ ë§Œë“¤ ìˆ˜ ìˆë‚˜ìš”?",
    "ì‚¬ì—° íˆ¬í‘œ": "ì œê°€ ëŒ€í™”í•œ ë‚´ìš©ì„ AI ì‚¬ì—° íˆ¬í‘œë¡œ ë§Œë“¤ ìˆ˜ ìˆë‚˜ìš”?",
    "ì¼ê¸°": "ì¼ê¸° ì‘ì„±ì´ë‚˜ ì €ì¥ì´ ê°€ëŠ¥í•œê°€ìš”?",
    "ë¬¸ì˜": "ëª¨ë‘íŠ¸ë¦¬ì— ë¬¸ì˜í•˜ê±°ë‚˜ ì˜ê²¬ì„ ë‚¨ê¸°ê³  ì‹¶ì–´ìš”.",
    "ì˜ê²¬": "ëª¨ë‘íŠ¸ë¦¬ì— ë¬¸ì˜í•˜ê±°ë‚˜ ì˜ê²¬ì„ ë‚¨ê¸°ê³  ì‹¶ì–´ìš”.",
    "ê²Œì‹œíŒ": "ëª¨ë‘íŠ¸ë¦¬ì— ë¬¸ì˜í•˜ê±°ë‚˜ ì˜ê²¬ì„ ë‚¨ê¸°ê³  ì‹¶ì–´ìš”.",
}

# --- LangGraph ìƒíƒœ ë° ë…¸ë“œ ---

class GraphState(TypedDict):
    uid: str
    message: str
    conversation_history: List[dict]
    intent: Literal["faq_check", "confirm_save_memo", "save_memo_execute", "general_chat"]
    final_response: str
    search_sources: Optional[List[dict]]
    has_search_results: bool

def determine_intent(state: GraphState) -> GraphState:
    """ì‚¬ìš©ì ì˜ë„ íŒŒì•…"""
    message = state["message"].lower()
    
    # ğŸš« FAQ/ë©”ëª¨ ì œì™¸ í‚¤ì›Œë“œ (ê²€ìƒ‰ìœ¼ë¡œ ë¶„ë¥˜)
    exclusion_keywords = [
        'ë°©ë²•', 'ì–´ë–»ê²Œ', 'ë­ì•¼', 'ë¬´ì—‡', 'êµ¬ê¸€', 'ë„¤ì´ë²„', 'ì¹´ì¹´ì˜¤',
        'ì¶”ì²œ', 'ì•Œë ¤ì¤˜', 'ì°¾ì•„ì¤˜', 'ê²€ìƒ‰', 'ì—ì„œ', 'ë¡œ', '~ëŠ”'
    ]
    
    has_exclusion = any(kw in message for kw in exclusion_keywords)
    
    if has_exclusion:
        print(f"[ì˜ë„ íŒŒì•…] ê²€ìƒ‰ í‚¤ì›Œë“œ ê°ì§€ â†’ general_chat")
        state["intent"] = "general_chat" 
        return state
    
    # 1. ë©”ëª¨ ì €ì¥ í‚¤ì›Œë“œ ì²´í¬ (ê¸°ì¡´ ì½”ë“œ)
    is_save_memo = any(keyword in message for keyword in [
        'ë©”ëª¨ë¡œ ë„£ì–´ì¤˜', 'ë©”ëª¨ ë„£ì–´ì¤˜', 'ë©”ëª¨ë¡œ ì €ì¥', 'ë©”ëª¨ ì €ì¥', 'ë©”ëª¨ ì¶”ê°€', 
        'ì¼ì • ì¶”ê°€', 'ì¼ì • ë“±ë¡', 'ê¸°ë¡ ì¶”ê°€', 'ê¸°ë¡ ì €ì¥', 'ì €ì¥í•´ì¤˜', 'ê¸°ë¡í•´ì¤˜', 'ë©”ëª¨ì €ì¥'
    ])
    
    if is_save_memo:
        state["intent"] = "confirm_save_memo"
        print(f"[ì˜ë„ íŒŒì•…] confirm_save_memo")
        return state

    # 2. âœ… FAQ í‚¤ì›Œë“œ ë§¤ì¹­ ê°œì„ 
    found_answers = []
    for keyword, question in FAQ_KEYWORD_MAP.items():
        if keyword in message:
            # FAQ_DATAì—ì„œ í•´ë‹¹ ì§ˆë¬¸ ì°¾ê¸°
            for item in FAQ_DATA:
                if item.get("question") == question:
                    found_answers.append(f"- {item.get('answer')}")
                    break
            break  # ì²« ë²ˆì§¸ ë§¤ì¹­ëœ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
    
    if found_answers:
        state["final_response"] = "\n\n".join(found_answers)
        state["intent"] = "faq_check"
        print(f"[ì˜ë„ íŒŒì•…] faq_check")
        return state

    # 3. ì¼ë°˜ ëŒ€í™”
    state["intent"] = "general_chat"
    print(f"[ì˜ë„ íŒŒì•…] general_chat")
    return state

def confirm_save_memo_agent(state: GraphState) -> GraphState:
    """ë©”ëª¨ ì €ì¥ í™•ì¸ ë©”ì‹œì§€ ë°˜í™˜"""
    state["final_response"] = "ë§ì”€í•˜ì‹  ë‚´ìš©ì„ ë©”ëª¨ë¡œ ì €ì¥í• ê¹Œìš”? ì•„ë‹ˆë©´ ë‹¤ë¥¸ ì§ˆë¬¸ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    print("[ë©”ëª¨ í™•ì¸] ì €ì¥ í™•ì¸ ìš”ì²­")
    return state

# ğŸš¨ ë©”ëª¨ ì €ì¥ ì‹¤í–‰ ë…¸ë“œ ê°œì„  (ë¡œê¹… ë° ì—ëŸ¬ ì‘ë‹µ ë¶„ë¦¬)
def execute_memo_agent(state: GraphState, uid) -> GraphState:
    """ë©”ëª¨ ì €ì¥ ì‹¤í–‰"""
    print("[LLM] ğŸ“ ë©”ëª¨ ì—ì´ì „íŠ¸ í˜¸ì¶œ")
    
    try:
        model = genai.GenerativeModel(
            model_name='gemini-2.0-flash-lite',
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": AGENT_SCHEMA,
                "temperature": 0.1,
            },
            system_instruction=SYSTEM_INSTRUCTION_MEMO_AGENT
        )
        
        # ì´ì „ ëŒ€í™” ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë©”ì‹œì§€ êµ¬ì„± 
        contents = [
             {"role": msg.get("role", "user"), "parts": [{"text": msg.get("content", "")}]} 
             for msg in state["conversation_history"]
        ]
        contents.append({"role": "user", "parts": [{"text": state["message"]}]})

        response = model.generate_content(contents) 
        raw_text = response.text
        
        json_match = re.search(r'\{[\s\S]*\}', raw_text)
        if not json_match:
            raise ValueError("AIê°€ ìœ íš¨í•œ JSONì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        response_data = json.loads(json_match.group(0))
        # ğŸš¨ LLM ì‘ë‹µ ë‚´ìš© ë¡œê¹… ì¶”ê°€
        print(f"[LLM ê²°ê³¼] ë©”ëª¨ ì—ì´ì „íŠ¸ JSON ì‘ë‹µ: {json.dumps(response_data, ensure_ascii=False, indent=2)}") 
        
        if db and response_data.get("action") == "SAVE_MEMO":
            memo_items = response_data.get("memoItems", [])
            saved_count = 0
            
            if not memo_items:
                # LLMì´ action=SAVE_MEMOë¥¼ ë°˜í™˜í–ˆì§€ë§Œ ì¶”ì¶œí•  ë‚´ìš©ì´ ì—†ë‹¤ê³  íŒë‹¨í•œ ê²½ìš°
                print("[ë©”ëª¨] âš ï¸ LLMì´ ì €ì¥í•  ë©”ëª¨ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (memoItems: []).")
                # â­ï¸ ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜: ì €ì¥í•  ë‚´ìš©ì´ ì—†ì„ ë•Œ ê³ ì •ëœ ë©”ì‹œì§€ ì‚¬ìš©
                response_data["userResponse"] = "ëŒ€í™” ë‚´ìš©ì—ì„œ ë©”ëª¨ë¡œ ì €ì¥í•  êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ì €ì¥í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?" 
            else:
                # ë©”ëª¨ í•­ëª©ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ DB ì €ì¥ ì‹œë„
                saved_count = save_memos_to_firestore(uid, memo_items, db)
            
                if saved_count == 0:
                    # DB ì €ì¥ ìì²´ì— ì‹¤íŒ¨í•œ ê²½ìš° (í•­ëª©ì€ ìˆì—ˆëŠ”ë° ì €ì¥ì´ ì•ˆë¨)
                    print("[ë©”ëª¨] âŒ ë©”ëª¨ í•­ëª©ì´ ìˆì—ˆìœ¼ë‚˜ DB ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (saved_count: 0)")
                    response_data["userResponse"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ë¡œ ë©”ëª¨ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                else:
                    print(f"[ë©”ëª¨] âœ… {saved_count}ê°œ ì €ì¥ ì™„ë£Œ")
        
        state["final_response"] = response_data.get("userResponse", "ë©”ëª¨ê°€ ì €ì¥ë˜ì—ˆì–´ìš”!")
        
    except Exception as e:
        print(f"[ë©”ëª¨] âŒ ì˜¤ë¥˜: {e}")
        state["final_response"] = "ë©”ëª¨ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
    return state

def call_general_chat_llm(state: GraphState) -> GraphState:
    """ì¼ë°˜ ëŒ€í™” LLM í˜¸ì¶œ (ê²€ìƒ‰ í†µí•©)"""
    print("[LLM] ğŸ’¬ ì¼ë°˜ ëŒ€í™” í˜¸ì¶œ (ê²€ìƒ‰ í†µí•©)")
    
    try:
        # 1. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ë¨¼ì €
        from search_api import classify_query, SearchCategory, perform_search
        category, clean_query = classify_query(state["message"])
        
        # 2. ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ ë¶„ê¸°
        if category == SearchCategory.CHAT:
            print("[ë¶„ê¸°] ğŸ’¬ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ")
            # ë°”ë¡œ Gemini ëŒ€í™”
            model = genai.GenerativeModel(
                model_name='gemini-2.0-flash-lite',
                system_instruction=SYSTEM_INSTRUCTION_PERSONA 
            )
            
            history = []
            # ëŒ€í™” ì´ë ¥ ì¤‘ ìµœê·¼ 3ê°œë§Œ í¬í•¨í•˜ì—¬ ë¬¸ë§¥ ìœ ì§€
            for msg in state["conversation_history"][-3:]: 
                role = msg.get("role", "user")
                content = msg.get("parts", [{}])[0].get("text", "") if "parts" in msg else msg.get("content", "")
                
                if content:
                    history.append({
                        "role": role,
                        "parts": [{"text": content}]
                    })
            
            chat = model.start_chat(history=history)
            response = chat.send_message(state["message"])
            state["final_response"] = response.text
            state["has_search_results"] = False
            
        else:
            print(f"[ë¶„ê¸°] ğŸ” ê²€ìƒ‰ ëª¨ë“œ ({category.value})")
            # ê²€ìƒ‰ ìˆ˜í–‰
            naver_id = os.environ.get("NAVER_CLIENT_ID")
            naver_secret = os.environ.get("NAVER_CLIENT_SECRET")
            serper_key = os.environ.get("SERPER_KEY")
                
            search_result = perform_search(
                state["message"], 
                genai,
                naver_id=naver_id,
                naver_secret=naver_secret,
                serper_key=serper_key
            )
                
            if search_result.get("success"):
                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ stateì— ì €ì¥
                state["final_response"] = search_result.get("summary", "")
                state["search_sources"] = search_result.get("sources", [])
                state["has_search_results"] = True
                print(f"[ê²€ìƒ‰] âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(state.get('search_sources', []))}ê°œ ì¶œì²˜")
            else:
                # ğŸ”¥ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ì—ëŸ¬ ë©”ì‹œì§€ë§Œ (Gemini ì‚¬ìš© ì•ˆ í•¨)
                print(f"[ê²€ìƒ‰] âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨, ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜")
                state["final_response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                state["has_search_results"] = False
        
    except Exception as e:
        print(f"[LLM] âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        state["final_response"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        state["has_search_results"] = False
    
    return state

def route_intent(state: GraphState) -> Literal["faq_check", "confirm_save_memo", "general_chat"]:
    """ì˜ë„ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
    if state["intent"] == "faq_check":
        return "faq_check"
    if state["intent"] == "confirm_save_memo":
        return "confirm_save_memo"
    return "general_chat"

# --- LangGraph ê·¸ë˜í”„ ë¹Œë“œ ---
workflow = StateGraph(GraphState)
workflow.add_node("determine_intent", determine_intent)
workflow.add_node("confirm_save_memo", confirm_save_memo_agent)
workflow.add_node("execute_memo_agent", execute_memo_agent)
workflow.add_node("call_general_chat_llm", call_general_chat_llm)

workflow.set_entry_point("determine_intent")

workflow.add_conditional_edges(
    "determine_intent",
    route_intent,
    {
        "faq_check": END,
        "confirm_save_memo": "confirm_save_memo",
        "general_chat": "call_general_chat_llm"
    }
)

workflow.add_edge("confirm_save_memo", END)
workflow.add_edge("execute_memo_agent", END)
workflow.add_edge("call_general_chat_llm", END)

memory_saver = MemorySaver()
app_graph = workflow.compile(checkpointer=memory_saver)
print("âœ… LangGraph ì´ˆê¸°í™” ì™„ë£Œ")

# --- FastAPI ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    token: str
    conversationHistory: List[dict]
    action: Optional[Literal["EXECUTE_MEMO", "GENERAL_CHAT"]] = None

class StreamRequest(BaseModel):
    query: str
    include_sources: Optional[bool] = True
    token: Optional[str] = None

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    """ë©”ì¸ ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    try:
        decoded_token = verify_firebase_token(request.token)
        uid = decoded_token["uid"]
    except HTTPException as e:
        raise e

    if not db:
        return {
            "success": False,
            "response": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ì œë¡œ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "remainingChats": 0
        }

    # âœ… ë¨¼ì € intent íŒŒì•… (í•œë„ ì°¨ê° ì—†ì´)
    needs_limit_check = False  # ê¸°ë³¸ê°’ ì„¤ì •
    remaining_chats = DAILY_CHAT_LIMIT
    
    try:
        # 1. intent íŒŒì•…ì„ ìœ„í•œ ì„ì‹œ state
        temp_state = GraphState(
            uid=uid,
            message=request.message,
            conversation_history=request.conversationHistory,
            intent="general_chat",
            final_response="",
            search_sources=[],
            has_search_results=False
        )
        
        # 2. actionì— ë”°ë¥¸ intent ì„¤ì •
        if request.action == "EXECUTE_MEMO":
            temp_state["intent"] = "save_memo_execute"
            needs_limit_check = False
        elif request.action == "GENERAL_CHAT":
            temp_state["intent"] = "general_chat"
            needs_limit_check = True
        else:
            # ì¼ë°˜ ì›Œí¬í”Œë¡œìš°: intent íŒŒì•…
            intent_result = determine_intent(temp_state)
            needs_limit_check = intent_result["intent"] == "general_chat"
        
        # 3. í•œë„ ì²´í¬ (ì¼ë°˜ ëŒ€í™”/ê²€ìƒ‰ë§Œ)
        if needs_limit_check:
            print(f"[í•œë„] ğŸ¦ ì¼ë°˜ ëŒ€í™”/ê²€ìƒ‰ - í•œë„ ì²´í¬ ì‹¤í–‰")
            try:
                limit_status = await check_and_update_chat_limit(uid)
                remaining_chats = limit_status["remainingChats"]

                if not limit_status["canChat"]:
                    return {
                        "success": False,
                        "response": "ì¼ì¼ ëŒ€í™” í•œë„(200íšŒ)ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.",
                        "remainingChats": 0
                    }
            except Exception as e:
                print(f"âŒ í•œë„ ì²´í¬ ì˜¤ë¥˜: {e}")
        else:
            print(f"[í•œë„] ğŸ†“ ë¬´ë£Œ ê¸°ëŠ¥ (FAQ/ë©”ëª¨) - í•œë„ ì²´í¬ ìƒëµ")

        # 4. ì‹¤ì œ ì²˜ë¦¬ ìˆ˜í–‰
        graph_input = GraphState(
            uid=uid,
            message=request.message,
            conversation_history=request.conversationHistory,
            intent="general_chat",
            final_response="",
            search_sources=[],
            has_search_results=False
        )
        
        config = {"configurable": {"thread_id": uid}}
        
        # âœ… actionì— ë”°ë¼ ë‹¤ë¥¸ ë…¸ë“œ ì‹¤í–‰
        if request.action == "EXECUTE_MEMO":
            # execute_memo_agent ë…¸ë“œë§Œ ì‹¤í–‰
            graph_input["intent"] = "save_memo_execute"
            final_state = execute_memo_agent(graph_input, uid)
        elif request.action == "GENERAL_CHAT":
            # general_chat ë…¸ë“œë§Œ ì‹¤í–‰
            final_state = call_general_chat_llm(graph_input)
        else:
            # ì •ìƒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = app_graph.invoke(graph_input, config=config)
        
        return {
            "success": True,
            "response": final_state["final_response"],
            "remainingChats": remaining_chats,
            "needsConfirmation": final_state.get("intent") == "confirm_save_memo",
            "hasSearchResults": final_state.get("has_search_results", False),
            "searchSources": final_state.get("search_sources", [])
        }

    except Exception as e:
        print(f"âŒ LangGraph ì˜¤ë¥˜: {e}")
        
        # ì˜¤ë¥˜ ì‹œ í•œë„ ë³µì› (í•œë„ê°€ ì°¨ê°ëœ ê²½ìš°ì—ë§Œ)
        if needs_limit_check and remaining_chats < DAILY_CHAT_LIMIT:
            try:
                today = date.today().isoformat()
                limit_ref = db.collection('users').document(uid).collection('limits').document(today)
                limit_ref.update({'count': firestore.Increment(-1)})
                remaining_chats += 1
                print(f"[í•œë„] ğŸ”„ ì˜¤ë¥˜ë¡œ ì¸í•œ í•œë„ ë³µì›")
            except Exception as restore_error:
                print(f"âŒ í•œë„ ë³µì› ì‹¤íŒ¨: {restore_error}")

        return {
            "success": False,
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "remainingChats": remaining_chats
        }

@app.get("/")
async def health_check():
    return {
        "status": "ok",
        "message": "Modoo Tree AI Chatbot API",
        "db_connected": db is not None
    }

@app.post("/stream")
async def stream_endpoint(request: StreamRequest):
    """FastAPI SSE ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸"""
    user_input = request.query.strip()
    
    if not user_input:
        raise HTTPException(status_code=400, detail="query í•„ìˆ˜")
    
    def generate_sse():
        start = time.time()
        
        try:
            # ===== 0ï¸âƒ£ ì¿¼ë¦¬ ì •ì œ ë° refresh íƒœê·¸ ê°ì§€ =====
            import re
            from search_api import clean_query as clean_query_func
            
            cleaned_query = clean_query_func(user_input)
            force_refresh = bool(re.search(r'\[refresh:\d+\]', user_input))
            if force_refresh:
                print(f"ğŸ”„ [FastAPI] ìºì‹œ ë¬´ì‹œ í”Œë˜ê·¸ ê°ì§€: '{user_input}' â†’ '{cleaned_query}'")
            
            # ===== 1ï¸âƒ£ ìºì‹œ í™•ì¸ (ì •ì œëœ ì¿¼ë¦¬ë¡œ, force_refreshê°€ Falseì¼ ë•Œë§Œ) =====
            cached_result = memory_cache.get(cleaned_query) if not force_refresh else None
            if cached_result:
                yield sse_format({
                    "stage": "cache",
                    "status": "hit",
                    "message": "ğŸ’¾ ìºì‹œëœ ê²°ê³¼ ë°˜í™˜ ì¤‘..."
                })
                
                # ìºì‹œëœ ìš”ì•½ì„ ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ë°˜í™˜
                summary = cached_result.get("summary", "")
                if summary:
                    # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
                    chunks = [summary[i:i+20] for i in range(0, len(summary), 20)]
                    for chunk in chunks:
                        yield sse_format({
                            "stage": "synthesis",
                            "status": "streaming",
                            "partial_answer": chunk
                        })
                        time.sleep(0.02)  # ìì—°ìŠ¤ëŸ¬ìš´ ì†ë„
                
                yield sse_format({
                    "stage": "complete",
                    "status": "success",
                    "summary": summary,
                    "sources": cached_result.get("sources", []),
                    "elapsed": time.time() - start,
                    "from_cache": True
                })
                return
            
            # ===== 1ï¸âƒ£ ì¿¼ë¦¬ ë¶„ë¥˜ =====
            yield sse_format({
                "stage": "classify",
                "status": "started",
                "message": "ğŸ” ê²€ìƒ‰ ì¤€ë¹„ ì¤‘..."
            })
            
            from search_api import classify_query, SearchCategory, perform_search
            category, clean_query = classify_query(user_input)
            
            yield sse_format({
                "stage": "classify",
                "status": "finished",
                "category": category.value,
                "message": f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {category.value}"
            })

            # ===== 2ï¸âƒ£ CHAT vs SEARCH ë¶„ê¸° =====
            if category == SearchCategory.CHAT:
                yield sse_format({
                    "stage": "chat",
                    "status": "started",
                    "message": "ğŸ’¬ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ"
                })
                
                # Gemini ëŒ€í™”
                model = genai.GenerativeModel(
                    model_name='gemini-2.0-flash-lite',
                    system_instruction=SYSTEM_INSTRUCTION_PERSONA
                )
                
                response = model.generate_content(user_input)
                final_answer = response.text
                
                yield sse_format({
                    "stage": "complete",
                    "status": "finished",
                    "category": category.value,
                    "duration_sec": round(time.time() - start, 2),
                    "answer_summary": final_answer,
                    "sources": [],
                    "message": f"âœ… ì¼ë°˜ ëŒ€í™” ì™„ë£Œ"
                })
                
                # ìºì‹œì— ì €ì¥ (ëŒ€í™”ëŠ” ì§§ì€ TTL)
                cache_data = {
                    "summary": final_answer,
                    "sources": [],
                    "category": category.value
                }
                memory_cache.set(cleaned_query, cache_data)
                return
                
            else:
                # ===== ê²€ìƒ‰ ëª¨ë“œ =====
                yield sse_format({
                    "stage": "search",
                    "status": "started", 
                    "message": f"ğŸ” {category.value} ê²€ìƒ‰ ì¤‘...",
                    "progress": 10
                })
                
                naver_id = os.environ.get("NAVER_CLIENT_ID")
                naver_secret = os.environ.get("NAVER_CLIENT_SECRET")
                serper_key = os.environ.get("SERPER_KEY")
                
                search_result = perform_search(
                    user_input, 
                    genai,
                    naver_id=naver_id,
                    naver_secret=naver_secret,
                    serper_key=serper_key
                )
                
                if search_result.get("success"):
                    yield sse_format({
                        "stage": "complete",
                        "status": "finished",
                        "category": category.value,
                        "duration_sec": round(time.time() - start, 2),
                        "answer_summary": search_result.get("summary", ""),
                        "sources": search_result.get("sources", []),
                        "message": f"âœ… ê²€ìƒ‰ ì™„ë£Œ"
                    })
                    
                    # ìºì‹œì— ì €ì¥
                    memory_cache.set(cleaned_query, search_result)
                else:
                    # ğŸ”¥ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ì—ëŸ¬ ë©”ì‹œì§€ë§Œ (Gemini ì‚¬ìš© ì•ˆ í•¨)
                    yield sse_format({
                        "stage": "complete",
                        "status": "finished",
                        "category": "error",
                        "duration_sec": round(time.time() - start, 2),
                        "answer_summary": "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ê²€ìƒ‰ ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                        "sources": [],
                        "message": f"âš ï¸ ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì˜¤ë¥˜"
                    })

        except Exception as e:
            trace = traceback.format_exc()
            print(f"âŒ FastAPI SSE ì—ëŸ¬: {e}\n{trace}")
            yield sse_format({
                "stage": "error",
                "error": str(e),
                "message": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}"
            })

    return StreamingResponse(
        generate_sse(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )

@app.get("/health")
async def health():
    return {"status": "healthy"}

# ===== Flask ì•± (SSE ìŠ¤íŠ¸ë¦¬ë°ìš©) =====
flask_app = Flask(__name__)

# CORS ì„¤ì • (ëª¨ë“  ì¶œì²˜ í—ˆìš©)
CORS(flask_app, 
     resources={r"/*": {"origins": "*"}},
     allow_headers=["Content-Type", "Authorization"],
     methods=["GET", "POST", "OPTIONS"],
     expose_headers=["Content-Type"],
     max_age=3600)

def sse_format(data: dict) -> str:
    """SSE í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

@flask_app.route("/stream", methods=["POST", "OPTIONS"])
def stream_search():
    """SSE ìŠ¤íŠ¸ë¦¬ë° ê²€ìƒ‰"""
    # OPTIONS ìš”ì²­ ì²˜ë¦¬ (CORS preflight)
    if request.method == "OPTIONS":
        print("ğŸ“¨ OPTIONS ìš”ì²­ ìˆ˜ì‹ ")
        response = Response("", status=200)
        response.headers["Access-Control-Allow-Origin"] = "*"
        response.headers["Access-Control-Allow-Methods"] = "POST, OPTIONS"
        response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization"
        response.headers["Access-Control-Max-Age"] = "3600"
        return response

    print(f"ğŸ“¨ POST ìš”ì²­ ìˆ˜ì‹ ")
    req = request.get_json(silent=True) or {}
    user_input = req.get("query", "").strip()
    
    if not user_input:
        return Response(
            sse_format({"error": "query í•„ìˆ˜"}),
            mimetype="text/event-stream"
        )

    def generate():
        start = time.time()
        
        try:
            # ===== 0ï¸âƒ£ ì¿¼ë¦¬ ì •ì œ ë° refresh íƒœê·¸ ê°ì§€ =====
            import re
            from search_api import clean_query as clean_query_func
            
            cleaned_query = clean_query_func(user_input)
            force_refresh = bool(re.search(r'\[refresh:\d+\]', user_input))
            if force_refresh:
                print(f"ğŸ”„ [Flask] ìºì‹œ ë¬´ì‹œ í”Œë˜ê·¸ ê°ì§€: '{user_input}' â†’ '{cleaned_query}'")
            
            # ===== 1ï¸âƒ£ ìºì‹œ í™•ì¸ (ì •ì œëœ ì¿¼ë¦¬ë¡œ, force_refreshê°€ Falseì¼ ë•Œë§Œ) =====
            cached_result = memory_cache.get(cleaned_query) if not force_refresh else None
            if cached_result:
                yield sse_format({
                    "stage": "cache",
                    "status": "hit",
                    "message": "ğŸ’¾ ìºì‹œëœ ê²°ê³¼ ë°˜í™˜ ì¤‘..."
                })
                
                # ìºì‹œëœ ìš”ì•½ì„ ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœë¡œ ë°˜í™˜
                summary = cached_result.get("summary", "")
                if summary:
                    # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
                    chunks = [summary[i:i+20] for i in range(0, len(summary), 20)]
                    for chunk in chunks:
                        yield sse_format({
                            "stage": "synthesis",
                            "status": "streaming",
                            "partial_answer": chunk
                        })
                        time.sleep(0.02)  # ìì—°ìŠ¤ëŸ¬ìš´ ì†ë„
                
                yield sse_format({
                    "stage": "complete",
                    "status": "success",
                    "summary": summary,
                    "sources": cached_result.get("sources", []),
                    "elapsed": time.time() - start,
                    "from_cache": True
                })
                return
            
            # ===== 1ï¸âƒ£ ì¿¼ë¦¬ ë¶„ë¥˜ =====
            yield sse_format({
                "stage": "classify",
                "status": "started",
                "message": "ğŸ” ê²€ìƒ‰ ì¤€ë¹„ ì¤‘..."
            })
            
            from search_api import classify_query, SearchCategory, perform_search
            category, clean_query = classify_query(user_input)
            
            yield sse_format({
                "stage": "classify",
                "status": "finished",
                "category": category.value,
                "message": f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {category.value}"
            })

            # ===== 2ï¸âƒ£ CHAT vs SEARCH ë¶„ê¸° =====
            if category == SearchCategory.CHAT:
                yield sse_format({
                    "stage": "chat",
                    "status": "started",
                    "message": "ğŸ’¬ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ"
                })
                
                # Gemini ëŒ€í™”
                model = genai.GenerativeModel(
                    model_name='gemini-2.0-flash-lite',
                    system_instruction=SYSTEM_INSTRUCTION_PERSONA
                )
                
                response = model.generate_content(user_input)
                final_answer = response.text
                
                yield sse_format({
                    "stage": "complete",
                    "status": "finished",
                    "category": category.value,
                    "duration_sec": round(time.time() - start, 2),
                    "answer_summary": final_answer,
                    "sources": [],
                    "message": f"âœ… ì¼ë°˜ ëŒ€í™” ì™„ë£Œ"
                })
                
                # ìºì‹œì— ì €ì¥ (ëŒ€í™”ëŠ” ì§§ì€ TTL)
                cache_data = {
                    "summary": final_answer,
                    "sources": [],
                    "category": category.value
                }
                memory_cache.set(cleaned_query, cache_data)
                return
                
            else:
                # ===== ê²€ìƒ‰ ëª¨ë“œ =====
                yield sse_format({
                    "stage": "search",
                    "status": "started", 
                    "message": f"ğŸ” {category.value} ê²€ìƒ‰ ì¤‘...",
                    "progress": 10
                })
                
                naver_id = os.environ.get("NAVER_CLIENT_ID")
                naver_secret = os.environ.get("NAVER_CLIENT_SECRET")
                serper_key = os.environ.get("SERPER_KEY")
                
                search_result = perform_search(
                    user_input, 
                    genai,
                    naver_id=naver_id,
                    naver_secret=naver_secret,
                    serper_key=serper_key
                )
                
                if search_result.get("success"):
                    yield sse_format({
                        "stage": "complete",
                        "status": "finished",
                        "category": category.value,
                        "duration_sec": round(time.time() - start, 2),
                        "answer_summary": search_result.get("summary", ""),
                        "sources": search_result.get("sources", []),
                        "message": f"âœ… ê²€ìƒ‰ ì™„ë£Œ"
                    })
                    
                    # ìºì‹œì— ì €ì¥
                    memory_cache.set(cleaned_query, search_result)
                else:
                    yield sse_format({
                        "stage": "error",
                        "error": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                    })

        except Exception as e:
            trace = traceback.format_exc()
            print(f"âŒ SSE ì—ëŸ¬: {e}\n{trace}")
            yield sse_format({
                "stage": "error",
                "error": str(e),
                "message": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)[:100]}"
            })

    return Response(
        generate(),
        mimetype="text/event-stream",
        headers={
            "Access-Control-Allow-Origin": "*",
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
        }
    )

@flask_app.route("/health", methods=["GET"])
def flask_health_check():
    """Flask ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    cache_stats = memory_cache.get_stats()
    return jsonify({
        "status": "ok",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "gemini": genai is not None,
            "naver": os.environ.get("NAVER_CLIENT_ID") is not None,
            "serper": os.environ.get("SERPER_KEY") is not None,
        },
        "cache": cache_stats
    }), 200

@flask_app.route("/cache/clear", methods=["POST"])
def clear_cache():
    """ìºì‹œ ìˆ˜ë™ ì‚­ì œ (ê´€ë¦¬ììš©)"""
    memory_cache.clear()
    return jsonify({"message": "ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}), 200

@flask_app.route("/cache/stats", methods=["GET"])
def cache_stats():
    """ìºì‹œ í†µê³„"""
    return jsonify(memory_cache.get_stats()), 200

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    
    # ê°œë°œ í™˜ê²½ì—ì„œëŠ” Flask ì‹¤í–‰ (SSE ì§€ì›)
    if os.environ.get("FLASK_ENV") == "development":
        print("ğŸš€ Flask ê°œë°œ ì„œë²„ë¡œ ì‹¤í–‰ (SSE ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)...")
        flask_app.run(host="0.0.0.0", port=port, debug=False, threaded=True)
    else:
        # í”„ë¡œë•ì…˜ì—ì„œëŠ” FastAPI ì‹¤í–‰ (ê¸°ë³¸ ì±„íŒ…)
        print("ğŸš€ FastAPI í”„ë¡œë•ì…˜ ì„œë²„ë¡œ ì‹¤í–‰...")
        uvicorn.run(app, host="0.0.0.0", port=port)
