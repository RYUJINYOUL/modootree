import requests
import json
import traceback
import time
import re
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
from typing import Dict, List, Tuple, Optional
from enum import Enum

# Trafilatura for fast web scraping
try:
    import trafilatura
    HAS_TRAFILATURA = True
    print("âœ… Trafilatura ë¡œë“œ ì™„ë£Œ")
except ImportError:
    HAS_TRAFILATURA = False
    print("âš ï¸ Trafilaturaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install trafilatura")

def clean_query(query: str) -> str:
    """
    ì¿¼ë¦¬ì—ì„œ ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
    - [refresh:íƒ€ì„ìŠ¤íƒ¬í”„] ì œê±°
    - ê³µë°± ì •ë¦¬
    """
    # [refresh:ìˆ«ì] íŒ¨í„´ ì œê±°
    cleaned = re.sub(r'\[refresh:\d+\]', '', query)
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    cleaned = re.sub(r'\s+', ' ', cleaned)
    # ì•ë’¤ ê³µë°± ì œê±°
    cleaned = cleaned.strip()
    
    if query != cleaned:
        print(f"ğŸ§¹ ì¿¼ë¦¬ ì •ì œ: '{query}' â†’ '{cleaned}'")
    
    return cleaned

class SearchCategory(Enum):
    RESTAURANT = "restaurant"
    CAFE = "cafe"
    ACCOMMODATION = "accommodation"
    SHOPPING = "shopping"
    NEWS = "news"
    PRODUCT = "product"
    ACTIVITY = "activity"
    VIDEO = "video"
    MUSIC = "music"
    GENERAL = "general"

def classify_query(query: str) -> Tuple[SearchCategory, str]:
    """ì¿¼ë¦¬ ë¶„ë¥˜ (ê²€ìƒ‰ ì „ìš©)"""
    # âœ… 1. ë¨¼ì € refresh íƒœê·¸ ì œê±°
    clean_q = clean_query(query)
    q = clean_q.lower()
    
    if query != clean_q:
        print(f"[ì¿¼ë¦¬ ì •ì œ] ì›ë³¸: '{query}' â†’ ì •ì œë¨: '{clean_q}'")
    
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ë§¤ì¹­ (í™•ì¥ëœ ë²”ìœ„)
    keywords = {
        SearchCategory.VIDEO: [
            "ìœ íŠœë¸Œ", "youtube", "ì˜ìƒ", "ë™ì˜ìƒ", "ë¹„ë””ì˜¤", "video", "ì˜í™”", "ë“œë¼ë§ˆ", 
            "ì˜ˆëŠ¥", "ë‹¤í", "ë¦¬ë·°", "íŠœí† ë¦¬ì–¼", "ê°•ì˜", "í´ë¦½", "ì‡¼ì¸ "
        ],
        SearchCategory.MUSIC: [
            "ë…¸ë˜", "ìŒì•…", "ë®¤ì§", "ê³¡", "song", "music", "ê°€ìˆ˜", "ì•„í‹°ìŠ¤íŠ¸", 
            "ì•¨ë²”", "ì‹±ê¸€", "ì°¨íŠ¸", "ë©œë¡ ", "ìŠ¤í¬í‹°íŒŒì´", "í”Œë ˆì´ë¦¬ìŠ¤íŠ¸"
        ],
        SearchCategory.RESTAURANT: [
            "ë§›ì§‘", "ìŒì‹ì ", "ë ˆìŠ¤í† ë‘", "ë¨¹ì„ê³³", "ì‹ë‹¹", "ìš”ë¦¬", "ìŒì‹", "ë©”ë‰´",
            "í•œì‹", "ì¤‘ì‹", "ì¼ì‹", "ì–‘ì‹", "ë¶„ì‹", "ì¹˜í‚¨", "í”¼ì", "í–„ë²„ê±°",
            "ì¹´í˜", "ë””ì €íŠ¸", "ë² ì´ì»¤ë¦¬", "ë¹µì§‘"
        ],
        SearchCategory.CAFE: [
            "ì¹´í˜", "ì»¤í”¼", "ë””ì €íŠ¸", "ë² ì´ì»¤ë¦¬", "ë¹µì§‘", "ìŠ¤íƒ€ë²…ìŠ¤", "ì´ë””ì•¼",
            "íˆ¬ì¸", "í• ë¦¬ìŠ¤", "ì»¤í”¼ë¹ˆ", "ë¼ë–¼", "ì•„ë©”ë¦¬ì¹´ë…¸", "ì¼€ì´í¬"
        ],
        SearchCategory.ACCOMMODATION: [
            "ìˆ™ì†Œ", "í˜¸í…”", "ëª¨í…”", "íœì…˜", "ë¦¬ì¡°íŠ¸", "ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤", "ì—ì–´ë¹„ì•¤ë¹„",
            "ë¯¼ë°•", "ì½˜ë„", "ìº í•‘", "ê¸€ë¨í•‘", "ì—¬ê´€", "ì°œì§ˆë°©"
        ],
        SearchCategory.NEWS: [
            "ë‰´ìŠ¤", "ê¸°ì‚¬", "ì†Œì‹", "ë³´ë„", "ì–¸ë¡ ", "ì‹ ë¬¸", "ë°©ì†¡", "ë‰´ìŠ¤ë£¸",
            "ì†ë³´", "í—¤ë“œë¼ì¸", "ì´ìŠˆ", "ì‚¬ê±´", "ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "ë¬¸í™”"
        ],
        SearchCategory.SHOPPING: [
            "ì‡¼í•‘", "êµ¬ë§¤", "ì˜¨ë¼ì¸ì‡¼í•‘", "ì¿ íŒ¡", "11ë²ˆê°€", "ì§€ë§ˆì¼“", "ì˜¥ì…˜",
            "ë„¤ì´ë²„ì‡¼í•‘", "ì•„ë§ˆì¡´", "ì´ë² ì´", "í• ì¸", "ì„¸ì¼", "íŠ¹ê°€"
        ],
        SearchCategory.PRODUCT: [
            "ì œí’ˆ", "ìƒí’ˆ", "ì¶”ì²œ", "ë¦¬ë·°", "í›„ê¸°", "í‰ì ", "ê°€ê²©", "ë¹„êµ",
            "ìŠ¤í™", "ì„±ëŠ¥", "ë¸Œëœë“œ", "ëª¨ë¸", "ì‹ ì œí’ˆ", "ë² ìŠ¤íŠ¸"
        ],
        SearchCategory.ACTIVITY: [
            "ì²´í—˜", "ê´€ê´‘", "ì—¬í–‰", "ë†€ê±°ë¦¬", "ë°ì´íŠ¸", "ë‚˜ë“¤ì´", "ì¶•ì œ", "ì´ë²¤íŠ¸",
            "ì „ì‹œ", "ê³µì—°", "ì½˜ì„œíŠ¸", "ë®¤ì§€ì»¬", "ì—°ê·¹", "ìŠ¤í¬ì¸ ", "ìš´ë™", "ì·¨ë¯¸"
        ],
        SearchCategory.GENERAL: [
            # ì¼ë°˜ì ì¸ ê²€ìƒ‰ ì˜ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë“¤
            "ì •ë³´", "ìë£Œ", "ë°ì´í„°", "ì•Œì•„ë³´ê¸°", "ì°¾ê¸°", "ê²€ìƒ‰", "ì¡°íšŒ",
            "í™•ì¸", "ë¬¸ì˜", "ì§ˆë¬¸", "ë‹µë³€", "í•´ê²°", "ë°©ë²•", "ê°€ì´ë“œ",
            "íŠœí† ë¦¬ì–¼", "ì„¤ëª…", "ì•ˆë‚´", "ë„ì›€", "ì§€ì›", "ì„œë¹„ìŠ¤"
        ]
    }
    
    for category, kws in keywords.items():
        if any(kw in q for kw in kws):
            # âœ… ì¶”ì²œ/ì•Œë ¤ì¤˜ ë“± ì œê±°
            final_clean = clean_q
            for word in ["ì¶”ì²œ", "ì•Œë ¤ì¤˜", "ì°¾ì•„ì¤˜", "ê²€ìƒ‰", "í•´ì¤˜"]:
                final_clean = final_clean.replace(word, "").strip()
            return category, final_clean
    
    return SearchCategory.GENERAL, clean_q

def fetch_api_data(source: str, query: str, naver_id: str = None, naver_secret: str = None, serper_key: str = None) -> Dict:
    """API ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    print(f"ğŸ” {source.upper()} ê²€ìƒ‰ ì‹œë„: '{query}' (naver_id: {bool(naver_id)}, serper_key: {bool(serper_key)})")
    
    try:
        if source == "naver" and naver_id and naver_secret:
            print(f"ğŸ“ ë„¤ì´ë²„ ë¡œì»¬ ê²€ìƒ‰ ì‹¤í–‰: {query}")
            r = requests.get(
                "https://openapi.naver.com/v1/search/local.json",
                headers={
                    "X-Naver-Client-Id": naver_id,
                    "X-Naver-Client-Secret": naver_secret,
                },
                params={"query": query, "display": 10},
                timeout=5
            )
            r.raise_for_status()
            result = r.json()
            print(f"âœ… ë„¤ì´ë²„ ê²€ìƒ‰ ì„±ê³µ: {len(result.get('items', []))}ê°œ ê²°ê³¼")
            return {"source": source, "data": result}
            
        elif source == "google" and serper_key:
            print(f"ğŸŒ êµ¬ê¸€(Serper) ê²€ìƒ‰ ì‹¤í–‰: {query}")
            r = requests.post(
                "https://google.serper.dev/search",
                headers={"X-API-KEY": serper_key, "Content-Type": "application/json"},
                json={"q": query, "num": 10},
                timeout=5
            )
            r.raise_for_status()
            result = r.json()
            print(f"âœ… êµ¬ê¸€ ê²€ìƒ‰ ì„±ê³µ: {len(result.get('organic', []))}ê°œ ê²°ê³¼")
            return {"source": source, "data": result}
            
        elif source == "youtube":
            try:
                from youtube_search import YoutubeSearch
                results = YoutubeSearch(query, max_results=10).to_dict()
                return {"source": source, "data": {"videos": results}}
            except ImportError:
                print("âš ï¸ youtube-search íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return {"source": source, "error": "youtube-search not installed"}
            
        else:
            return {"source": source, "error": "config not found"}
    
    except Exception as e:
        print(f"âš ï¸ {source.upper()} API ì—ëŸ¬: {e}")
        return {"source": source, "error": str(e)}
    
    # ğŸ”¥ ì¡°ê±´ì— ë§ì§€ ì•ŠëŠ” ê²½ìš° (ë„¤ì´ë²„ í‚¤ ì—†ìŒ, êµ¬ê¸€ í‚¤ ì—†ìŒ ë“±)
    print(f"âš ï¸ {source.upper()} ê²€ìƒ‰ ì¡°ê±´ ë¶ˆì¶©ì¡±: naver_id={bool(naver_id)}, serper_key={bool(serper_key)}")
    return {"source": source, "error": "API í‚¤ ë˜ëŠ” ì¡°ê±´ ë¶ˆì¶©ì¡±"}

def filter_search_results(raw_results: List[Dict]) -> List[Dict]:
    """ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ ë° ë§í¬ ì¶”ì¶œ"""
    cleaned = []
    
    for result in raw_results:
        source = result.get("source")
        data = result.get("data", {})
        
        if source == "naver":
            items = data.get("items", [])
            for item in items[:5]:
                title = re.sub(r'<[^>]+>', '', item.get("title", ""))
                desc = re.sub(r'<[^>]+>', '', item.get("description", ""))
                
                cleaned.append({
                    "source": source,
                    "title": title,
                    "link": item.get("link", ""),
                    "snippet": desc,
                    "address": item.get("address", ""),
                })
        
        elif source == "google":
            items = data.get("organic", [])
            for item in items[:5]:
                cleaned.append({
                    "source": source,
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                })
        
        elif source == "youtube":
            videos = data.get("videos", [])
            for video in videos[:5]:
                cleaned.append({
                    "source": source,
                    "title": video.get("title", ""),
                    "link": f"https://www.youtube.com{video.get('url_suffix', '')}",
                    "snippet": f"{video.get('channel', '')} Â· {video.get('duration', '')} Â· {video.get('views', '')}",
                    "channel": video.get("channel", ""),
                    "duration": video.get("duration", ""),
                    "views": video.get("views", ""),
                    "thumbnail": f"https://i.ytimg.com/vi/{video.get('id', '')}/hqdefault.jpg" if video.get('id') else ""
                })
    
    return cleaned

def scrape_page(url: str, max_chars: int = 500) -> Dict:
    """ë‹¨ì¼ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ (Trafilatura ì‚¬ìš©)"""
    if not HAS_TRAFILATURA:
        return {
            "url": url,
            "summary": "ìŠ¤í¬ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.",
            "success": False
        }
    
    try:
        response = requests.get(url, timeout=5, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()
        
        text = trafilatura.extract(
            response.text,
            include_comments=False,
            include_tables=False,
            no_fallback=False
        )
        
        if not text or len(text.strip()) < 50:
            return {
                "url": url,
                "summary": "ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "success": False
            }
        
        summary = text[:max_chars].strip()
        if len(text) > max_chars:
            summary += "..."
        
        return {
            "url": url,
            "summary": summary,
            "full_text": text[:1500],
            "success": True
        }
    
    except Exception as e:
        print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ({url}): {e}")
        return {
            "url": url,
            "summary": f"í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)[:50]}",
            "success": False
        }

def scrape_multiple_pages(urls: List[str], max_workers: int = 5) -> List[Dict]:
    """ë³‘ë ¬ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {
            executor.submit(scrape_page, url): url 
            for url in urls[:10]
        }
        
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                result = future.result(timeout=7)
                results.append(result)
            except Exception as e:
                print(f"âŒ ìŠ¤í¬ë˜í•‘ íƒ€ì„ì•„ì›ƒ: {url}")
                results.append({
                    "url": url,
                    "summary": "íƒ€ì„ì•„ì›ƒ",
                    "success": False
                })
    
    return results

def perform_search(query: str, genai_client, naver_id: str = None, naver_secret: str = None, serper_key: str = None) -> Dict:
    """í†µí•© ê²€ìƒ‰ ìˆ˜í–‰"""
    try:
        # 1. ì¿¼ë¦¬ ë¶„ë¥˜ (classify_query ë‚´ë¶€ì—ì„œ clean_query í˜¸ì¶œ)
        category, final_query = classify_query(query)
        print(f"[ê²€ìƒ‰] ì¹´í…Œê³ ë¦¬: {category.value}, ì¿¼ë¦¬: {final_query}")
        
        # 2. ê²€ìƒ‰ ì†ŒìŠ¤ ì„ íƒ
        search_sources = []
        if category in [SearchCategory.VIDEO, SearchCategory.MUSIC]:
            search_sources = ["youtube", "google"]
        else:
            # ğŸ”¥ ë„¤ì´ë²„ ìš°ì„ , êµ¬ê¸€ ë°±ì—…
            if naver_id and naver_secret:
                search_sources.append("naver")
            if serper_key:
                search_sources.append("google")
            
            # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì—ëŸ¬
            if not search_sources:
                return {
                    "success": False,
                    "error": "ê²€ìƒ‰ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                }
        
        # 3. ë³‘ë ¬ ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„: ë„¤ì´ë²„ â†’ êµ¬ê¸€)
        raw_results = []
        print(f"ğŸš€ ê²€ìƒ‰ ì†ŒìŠ¤: {search_sources}")
        
        with ThreadPoolExecutor(max_workers=3) as ex:
            futures = []
            
            # ğŸ”¥ ë„¤ì´ë²„ ìš°ì„  ì‹¤í–‰ (API í‚¤ ì²´í¬ ê°•í™”)
            if "naver" in search_sources:
                if naver_id and naver_secret:
                    print(f"âœ… ë„¤ì´ë²„ ê²€ìƒ‰ íì— ì¶”ê°€: ID={naver_id[:4]}..., SECRET={bool(naver_secret)}")
                    futures.append(ex.submit(fetch_api_data, "naver", final_query, naver_id, naver_secret, None))
                else:
                    print(f"âŒ ë„¤ì´ë²„ API í‚¤ ëˆ„ë½: ID={bool(naver_id)}, SECRET={bool(naver_secret)}")
            
            # êµ¬ê¸€ ì‹¤í–‰
            if "google" in search_sources:
                if serper_key:
                    print(f"âœ… êµ¬ê¸€ ê²€ìƒ‰ íì— ì¶”ê°€: KEY={serper_key[:10]}...")
                    futures.append(ex.submit(fetch_api_data, "google", final_query, None, None, serper_key))
                else:
                    print(f"âŒ Serper API í‚¤ ëˆ„ë½")
            
            # ìœ íŠœë¸Œ ì‹¤í–‰
            if "youtube" in search_sources:
                print(f"âœ… ìœ íŠœë¸Œ ê²€ìƒ‰ íì— ì¶”ê°€")
                futures.append(ex.submit(fetch_api_data, "youtube", final_query, None, None, None))
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for i, future in enumerate(futures):
                try:
                    result = future.result(timeout=10)  # íƒ€ì„ì•„ì›ƒ ëŠ˜ë¦¼
                    raw_results.append(result)
                    print(f"ğŸ“¦ ê²€ìƒ‰ ê²°ê³¼ {i+1}/{len(futures)} ìˆ˜ì§‘: {result.get('source', 'unknown')}")
                except TimeoutError:
                    print(f"â° ê²€ìƒ‰ {i+1} API íƒ€ì„ì•„ì›ƒ")
                except Exception as e:
                    print(f"âŒ ê²€ìƒ‰ {i+1} ì˜ˆì™¸: {e}")
        
        # 4. ê²°ê³¼ í•„í„°ë§
        print(f"ğŸ“¦ raw_results: {json.dumps(raw_results, ensure_ascii=False, indent=2)}")
        cleaned = filter_search_results(raw_results)
        print(f"âœ… cleaned ê²°ê³¼: {len(cleaned)}ê°œ")
        
        if not cleaned:
            print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ. raw_results ìƒì„¸:")
            for r in raw_results:
                print(f"  - source: {r.get('source')}, error: {r.get('error')}, data keys: {list(r.get('data', {}).keys())}")
            
            return {
                "success": False,
                "error": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "debug_info": {
                    "raw_count": len(raw_results),
                    "raw_sources": [r.get('source') for r in raw_results],
                    "errors": [r.get('error') for r in raw_results if r.get('error')]
                }
            }
        
        # 5. í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
        scraped_data = []
        if HAS_TRAFILATURA:
            links = [item["link"] for item in cleaned if item.get("link")]
            scraped_data = scrape_multiple_pages(links, max_workers=5)
        
        # 6. LLM ìš”ì•½
        context_data = []
        for item in scraped_data:
            if item["success"]:
                context_data.append({
                    "url": item["url"],
                    "content": item["full_text"]
                })
        
        for item in cleaned[:10]:
            context_data.append({
                "title": item.get("title", ""),
                "snippet": item.get("snippet", ""),
                "url": item.get("link", "")
            })
        
        prompt = f"""ì‚¬ìš©ì ì¿¼ë¦¬: {query}

ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì´ê³  ëª…í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”:

{json.dumps(context_data[:10], ensure_ascii=False, indent=2)}

ë‹µë³€ í˜•ì‹:
- 5~7ê°œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±
- í•µì‹¬ ì •ë³´ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½
- ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´
- êµ¬ì²´ì ì¸ ì •ë³´ í¬í•¨ (ì£¼ì†Œ, ê°€ê²©, í‰ì  ë“±)"""
        
        # LLM í˜¸ì¶œ
        import google.generativeai as genai
        model = genai.GenerativeModel(
            model_name='gemini-2.0-flash',
            generation_config={
                "temperature": 0.3,
                "max_output_tokens": 600
            }
        )
        
        response = model.generate_content(prompt)
        summary = response.text
        
        # 7. ê²°ê³¼ ë°˜í™˜
        return {
            "success": True,
            "summary": summary,
            "sources": [
                {
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", "")[:150],
                    "link": item.get("link", ""),
                    "source": item.get("source", "")
                }
                for item in cleaned[:10]
            ],
            "category": category.value
        }
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e)
        }

